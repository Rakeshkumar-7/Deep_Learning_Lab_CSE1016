{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Reg. no: 20BAI1055\n",
        "### Name: Rakesh Kumar K S\n",
        "### CSE1016 Lab assignment - 8\n",
        "#### - Create an RNN that can write like shakespeare (finished in lab)\n",
        "#### - Using what you learnt, train a model to reproduce text from a different dataset(say CBSE text books or any other text book)\n",
        "\n",
        "I've decided to train the RNN to reproduce stories\n",
        "\n",
        "Source of the stories text file: http://www.textfiles.com/stories/aesop11.txt"
      ],
      "metadata": {
        "id": "UYpQEvSDpIRn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "Fnw10Fjvo7ld"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --fuzzy https://drive.google.com/file/d/1II2iXWWfI1--N51ZgyY9Px1Onu1PnlzE/view?usp=sharing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PguAg0-fP9GC",
        "outputId": "e8835d43-303a-4c27-e1f6-ac66797caf7c"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1II2iXWWfI1--N51ZgyY9Px1Onu1PnlzE\n",
            "To: /content/lab8_stories.txt\n",
            "\r  0% 0.00/229k [00:00<?, ?B/s]\r100% 229k/229k [00:00<00:00, 104MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open('lab8_stories.txt', 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2g-ESFsnqRUV",
        "outputId": "907ea263-b2c0-4e07-c4f2-40bef535e74d"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 229320 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aammqv0BqW26",
        "outputId": "89b491ec-f46d-49ad-bf22-c9210b41ecbd"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\n",
            "Aesop's Fables Translated by George Fyler Townsend \r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            " \r\n",
            "The Wolf and the Lamb \r\n",
            " \r\n",
            "WOLF, meeting with a Lamb astray from the fold, resolved not to\r\n",
            "lay violent hands on him, but to find some plea to justify to the\r\n",
            "Lamb the Wolf's right to ea\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQOciZyNqYAj",
        "outputId": "614ca61e-4c79-453f-ba9f-d5f2a9573c5d"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "78 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74ii7szWqZ4L",
        "outputId": "f32f64c0-f29f-4c43-eb72-fa4a283b3365"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ],
      "metadata": {
        "id": "a9hV439nqbcm"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CfL2-KNqceC",
        "outputId": "da6ef6ea-cdc8-48a3-893a-3d9b37578748"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[53, 54, 55, 56, 57, 58, 59], [76, 77, 78]]>"
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "V4xsxKrwqd_c"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8-MRHHGqfv6",
        "outputId": "b5908469-ccc0-4da7-dfcc-857c695c6f88"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPlqKGuuqhDb",
        "outputId": "38280296-d665-41e5-f652-f8b309b74f10"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "ZUAD_3-CqiHz"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pa68EAcDqjJ8",
        "outputId": "b45dde3d-181b-42ce-80ab-f73596079926"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(229320,), dtype=int64, numpy=array([ 2,  1, 27, ..., 13,  2,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "tEqM61QGqked"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oydjXgtkqltU",
        "outputId": "fd72911b-00a0-451e-d009-505557fe6048"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\n",
            "\n",
            "\n",
            "A\n",
            "e\n",
            "s\n",
            "o\n",
            "p\n",
            "'\n",
            "s\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100"
      ],
      "metadata": {
        "id": "LpUdmZ0aqmhL"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOULhLIgqnmZ",
        "outputId": "503b03d6-22a7-40f9-8e04-f308dcf547bb"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'\\r' b'\\n' b'A' b'e' b's' b'o' b'p' b\"'\" b's' b' ' b'F' b'a' b'b' b'l'\n",
            " b'e' b's' b' ' b'T' b'r' b'a' b'n' b's' b'l' b'a' b't' b'e' b'd' b' '\n",
            " b'b' b'y' b' ' b'G' b'e' b'o' b'r' b'g' b'e' b' ' b'F' b'y' b'l' b'e'\n",
            " b'r' b' ' b'T' b'o' b'w' b'n' b's' b'e' b'n' b'd' b' ' b'\\r' b'\\n' b'\\r'\n",
            " b'\\n' b'\\r' b'\\n' b'\\r' b'\\n' b' ' b'\\r' b'\\n' b'T' b'h' b'e' b' ' b'W'\n",
            " b'o' b'l' b'f' b' ' b'a' b'n' b'd' b' ' b't' b'h' b'e' b' ' b'L' b'a'\n",
            " b'm' b'b' b' ' b'\\r' b'\\n' b' ' b'\\r' b'\\n' b'W' b'O' b'L' b'F' b',' b' '\n",
            " b'm' b'e' b'e' b't'], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IOZzZZHqoeb",
        "outputId": "2d483c48-094a-42e3-a657-931be2dc27a4"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b\"\\r\\nAesop's Fables Translated by George Fyler Townsend \\r\\n\\r\\n\\r\\n\\r\\n \\r\\nThe Wolf and the Lamb \\r\\n \\r\\nWOLF, meet\"\n",
            "b'ing with a Lamb astray from the fold, resolved not to\\r\\nlay violent hands on him, but to find some ple'\n",
            "b'a to justify to the\\r\\nLamb the Wolf\\'s right to eat him.  He thus addressed him:\\r\\n\"Sirrah, last year yo'\n",
            "b'u grossly insulted me.\"  \"Indeed,\" bleated\\r\\nthe Lamb in a mournful tone of voice, \"I was not then bor'\n",
            "b'n.\"  Then\\r\\nsaid the Wolf, \"You feed in my pasture.\"  \"No, good sir,\" replied\\r\\nthe Lamb, \"I have not y'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "Q6AHTDDyqqLb"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xHaUCHcqsNv",
        "outputId": "eb1eded1-fa88-4173-c233-534a4e1a29d2"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "JrH0xXGVqtgL"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmycxSkjqu3Z",
        "outputId": "c9f949c7-93b6-4706-fa29-5cb35c0d36b3"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b\"\\r\\nAesop's Fables Translated by George Fyler Townsend \\r\\n\\r\\n\\r\\n\\r\\n \\r\\nThe Wolf and the Lamb \\r\\n \\r\\nWOLF, mee\"\n",
            "Target: b\"\\nAesop's Fables Translated by George Fyler Townsend \\r\\n\\r\\n\\r\\n\\r\\n \\r\\nThe Wolf and the Lamb \\r\\n \\r\\nWOLF, meet\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LCOKhh7qwHD",
        "outputId": "4f5a57d8-3f0b-454f-bf76-b5f433581f3f"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "ZGYVxrObqxVV"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "8FyO85YLq1Kp"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "u1mhrH81q3fq"
      },
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hYbG7Ewq5MS",
        "outputId": "6490d62e-bc9d-4caa-d535-b6fe43f66ee3"
      },
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 79) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SUkfiJVq6if",
        "outputId": "62f06c05-0e05-4880-f958-f09c1d536448"
      },
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_6 (Embedding)     multiple                  20224     \n",
            "                                                                 \n",
            " gru_6 (GRU)                 multiple                  3938304   \n",
            "                                                                 \n",
            " dense_6 (Dense)             multiple                  80975     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,039,503\n",
            "Trainable params: 4,039,503\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "HDTurO7Iq7pF"
      },
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqmrS6Jhq9sP",
        "outputId": "0ab0e22a-25e4-46d8-a03a-1281d4132956"
      },
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([25, 36, 69, 64, 24, 71, 44,  7, 45,  9, 28, 75,  6, 69, 43, 55, 31,\n",
              "       54, 30, 39, 46, 72, 33, 29,  5, 25, 64,  1,  0, 51, 72, 44, 58, 33,\n",
              "       40, 77, 47, 63, 68, 20, 36, 78, 18, 16, 44,  1, 11, 75, 48,  6, 64,\n",
              "       22, 53, 24, 57, 77, 14, 12, 37, 61, 50, 67, 75, 39, 26, 33, 54, 54,\n",
              "       68, 54, 50, 43, 45, 21, 56, 16, 26, 28, 63, 61, 65, 19, 13, 78, 43,\n",
              "        8, 72, 56,  4, 74, 17, 32, 20,  5, 26, 45, 75, 50, 27, 33])"
            ]
          },
          "metadata": {},
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pb60lcKmrAAi",
        "outputId": "c0844b64-d238-4ad9-cb32-95352629a3bd"
      },
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b'ause, said, \"Pray do not grieve so; but\\r\\ngo and take a stone, and place it in the hole, and fancy th'\n",
            "\n",
            "Next Char Predictions:\n",
            " b';Jql:sR\\'S)Bw&qQcEbDMTtGC\";l\\n[UNK]YtRfGNyUkp6Jz42R\\n,wV&l8a:ey0-KiXowM?GbbpbXQS7d2?Bkim5.zQ(td!v3F6\"?SwXAG'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "IHWTn1FGrBfQ"
      },
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCsAJM8mrIok",
        "outputId": "05fb726d-5d6a-42d6-9b05-d8244060ca60"
      },
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 79)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.3706875, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUyls71wrJ__",
        "outputId": "c3bab573-729c-4da5-9518-7ddc2b2dec5d"
      },
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "79.09799"
            ]
          },
          "metadata": {},
          "execution_count": 223
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "XO13zICFrLJ0"
      },
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "Pl_IMF0TrMOq"
      },
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 50"
      ],
      "metadata": {
        "id": "1hIjMN9ZrQ4K"
      },
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wRwuhborSUA",
        "outputId": "7dd0381a-8be8-4a7d-e581-7fc73ae78b63"
      },
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "35/35 [==============================] - 4s 58ms/step - loss: 3.7256\n",
            "Epoch 2/50\n",
            "35/35 [==============================] - 2s 57ms/step - loss: 2.7524\n",
            "Epoch 3/50\n",
            "35/35 [==============================] - 2s 57ms/step - loss: 2.4169\n",
            "Epoch 4/50\n",
            "35/35 [==============================] - 2s 57ms/step - loss: 2.2533\n",
            "Epoch 5/50\n",
            "35/35 [==============================] - 2s 58ms/step - loss: 2.1414\n",
            "Epoch 6/50\n",
            "35/35 [==============================] - 2s 58ms/step - loss: 2.0482\n",
            "Epoch 7/50\n",
            "35/35 [==============================] - 2s 58ms/step - loss: 1.9611\n",
            "Epoch 8/50\n",
            "35/35 [==============================] - 2s 59ms/step - loss: 1.8818\n",
            "Epoch 9/50\n",
            "35/35 [==============================] - 2s 59ms/step - loss: 1.8085\n",
            "Epoch 10/50\n",
            "35/35 [==============================] - 2s 59ms/step - loss: 1.7400\n",
            "Epoch 11/50\n",
            "35/35 [==============================] - 2s 59ms/step - loss: 1.6766\n",
            "Epoch 12/50\n",
            "35/35 [==============================] - 2s 59ms/step - loss: 1.6186\n",
            "Epoch 13/50\n",
            "35/35 [==============================] - 2s 60ms/step - loss: 1.5585\n",
            "Epoch 14/50\n",
            "35/35 [==============================] - 2s 60ms/step - loss: 1.5026\n",
            "Epoch 15/50\n",
            "35/35 [==============================] - 2s 59ms/step - loss: 1.4477\n",
            "Epoch 16/50\n",
            "35/35 [==============================] - 2s 60ms/step - loss: 1.3909\n",
            "Epoch 17/50\n",
            "35/35 [==============================] - 2s 59ms/step - loss: 1.3389\n",
            "Epoch 18/50\n",
            "35/35 [==============================] - 2s 59ms/step - loss: 1.2890\n",
            "Epoch 19/50\n",
            "35/35 [==============================] - 2s 59ms/step - loss: 1.2384\n",
            "Epoch 20/50\n",
            "35/35 [==============================] - 2s 59ms/step - loss: 1.1893\n",
            "Epoch 21/50\n",
            "35/35 [==============================] - 2s 59ms/step - loss: 1.1379\n",
            "Epoch 22/50\n",
            "35/35 [==============================] - 2s 58ms/step - loss: 1.0829\n",
            "Epoch 23/50\n",
            "35/35 [==============================] - 2s 58ms/step - loss: 1.0316\n",
            "Epoch 24/50\n",
            "35/35 [==============================] - 2s 58ms/step - loss: 0.9742\n",
            "Epoch 25/50\n",
            "35/35 [==============================] - 2s 58ms/step - loss: 0.9133\n",
            "Epoch 26/50\n",
            "35/35 [==============================] - 2s 58ms/step - loss: 0.8516\n",
            "Epoch 27/50\n",
            "35/35 [==============================] - 2s 58ms/step - loss: 0.7893\n",
            "Epoch 28/50\n",
            "35/35 [==============================] - 2s 58ms/step - loss: 0.7229\n",
            "Epoch 29/50\n",
            "35/35 [==============================] - 2s 57ms/step - loss: 0.6574\n",
            "Epoch 30/50\n",
            "35/35 [==============================] - 2s 57ms/step - loss: 0.5953\n",
            "Epoch 31/50\n",
            "35/35 [==============================] - 2s 57ms/step - loss: 0.5304\n",
            "Epoch 32/50\n",
            "35/35 [==============================] - 2s 58ms/step - loss: 0.4702\n",
            "Epoch 33/50\n",
            "35/35 [==============================] - 2s 58ms/step - loss: 0.4112\n",
            "Epoch 34/50\n",
            "35/35 [==============================] - 2s 58ms/step - loss: 0.3596\n",
            "Epoch 35/50\n",
            "35/35 [==============================] - 2s 57ms/step - loss: 0.3146\n",
            "Epoch 36/50\n",
            "35/35 [==============================] - 2s 58ms/step - loss: 0.2743\n",
            "Epoch 37/50\n",
            "35/35 [==============================] - 2s 58ms/step - loss: 0.2359\n",
            "Epoch 38/50\n",
            "35/35 [==============================] - 2s 58ms/step - loss: 0.2063\n",
            "Epoch 39/50\n",
            "35/35 [==============================] - 2s 58ms/step - loss: 0.1811\n",
            "Epoch 40/50\n",
            "35/35 [==============================] - 2s 58ms/step - loss: 0.1601\n",
            "Epoch 41/50\n",
            "35/35 [==============================] - 2s 58ms/step - loss: 0.1428\n",
            "Epoch 42/50\n",
            "35/35 [==============================] - 2s 58ms/step - loss: 0.1270\n",
            "Epoch 43/50\n",
            "35/35 [==============================] - 2s 58ms/step - loss: 0.1146\n",
            "Epoch 44/50\n",
            "35/35 [==============================] - 2s 58ms/step - loss: 0.1054\n",
            "Epoch 45/50\n",
            "35/35 [==============================] - 2s 59ms/step - loss: 0.0982\n",
            "Epoch 46/50\n",
            "35/35 [==============================] - 2s 59ms/step - loss: 0.0919\n",
            "Epoch 47/50\n",
            "35/35 [==============================] - 2s 63ms/step - loss: 0.0876\n",
            "Epoch 48/50\n",
            "35/35 [==============================] - 2s 59ms/step - loss: 0.0836\n",
            "Epoch 49/50\n",
            "35/35 [==============================] - 2s 59ms/step - loss: 0.0804\n",
            "Epoch 50/50\n",
            "35/35 [==============================] - 2s 58ms/step - loss: 0.0778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "jiXsBSx1rTYR"
      },
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "kiJoDGK5rU7g"
      },
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['Story:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELtK0o83rWCW",
        "outputId": "3e99e435-4584-42bf-b620-9f9c202d6868"
      },
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Story:  \"Shalloo sove abreaditted, yot then might rish had been\r\n",
            "assigned by the asce digned at the sound and wondered what\r\n",
            "it forebode, when he was excellent before he\r\n",
            "had two lame far off on me?'  \r\n",
            " \r\n",
            "\r\n",
            "The Mountain in Labor \r\n",
            " \r\n",
            "A MOUSE who always lived on the land, by an unlore postible moved heale the\r\n",
            "more of an interest in the Pantrars, and recuining the\r\n",
            "hearty-for he wished to a place there you would, you shall have an amount of\r\n",
            "truth, and if the beasts of the forest fled at his approach.  The Lion\r\n",
            "alone challengly had between them.  When the\r\n",
            "Wolf came up to a Shepherd and fawned upon the preference over all other mythologists.  His\r\n",
            "'Mountain delivered of a Mouse.\"  \r\n",
            " \r\n",
            "1t stood un all over the\r\n",
            "marketplace.  One day an old hound said to him:  Why do you make\r\n",
            "such an exhibition of the ends.  The work of Jusidence, and when they saw the\r\n",
            "Ill said to him, \"O two bricks of a Heepon:  \"My nour leik bought My mached as Kinger, ase; by\r\n",
            "the great beautys of emprose of his grief,  \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.126692056655884\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['Story:', 'Story:', 'Story:', 'Story:', 'Story:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CA4WiLp4rXW9",
        "outputId": "c940d7f0-46e9-4953-a18d-50d68555f24a"
      },
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'Story:  \"Planu,\\r\\nmust having hands, for the man is in earnest this time; he no longer\\r\\ntrusts his friends, but became tangled in the meshes and\\r\\ndrowned.  With his last breaking his perfect more easily a\\r\\nsestoponor, and was placed in the library they and again consticuto to in actor as you are\\r\\ndea?\\'  Finners, are young, to give him nothing but a too  uf, simple inlice to the\\r\\nsheepfold.\"  \\r\\n \\r\\nEvil tenstle Debolies.  He seized him and\\r\\npurposely placed his wings and childs.  The Man rushed upon\\r\\nthe Cat,\" there was no one found to do it.  \\r\\n \\r\\n\\r\\nThe Wolf and the Goat \\r\\n \\r\\nA WOLF saw some Fles and decorn:  he\\r\\nplaced the Horseman in chalos or a complaint to Jupiter, said\\r\\nto a File, and asked of him the favor of a pitcher, who half from the\\r\\nenoborite of a fable involved in as safe younges, who have escaped\\r\\nfrom man, only to throw myself in the wite which of them whose heard the\\r\\nanimals of the field, the birds of the nuteratury.  \\r\\n \\r\\n\\r\\nThe Wolf and the Goat \\r\\n \\r\\nA WOLF saw some Shepherds'\n",
            " b'Story:  wal remused still in the broke, \\r\\n \\r\\nThe Goat and the Aesop \\r\\n \\r\\nAN EAGLE and a Fox formed an intimate first of all\\r\\nkept to come, and leaving him to ad his perchance in the hear of a\\r\\nwhelp remoment of Lidea y.u Goth and at the same upbearing, whereon\\r\\nhim taking an account of the load, you altoged yourself?  It was an another\\r\\nand two Mench-sawed with all his might.  An Eagle saluted his color and publication\\r\\nof Aesop, by account of the opine of fear, at his deniarb, and easily insere\\r\\nof your name.  The Wolf, being thus fearfully much amuses ground, and being demand to eating the\\r\\nAps on the Apes, enraged at hearing the sun shine Bott my entreatien, and the\\r\\nother to a fitching from the first chuld on the noble of its\\r\\nsong.  When the time came for killing the Goose, the cook went to\\r\\nget him at night, when she letter of a mortal\\r\\ncombat.  When they stopped suddenly to catch their breash and drive him \\r\\n \\r\\n\\r\\nThe Lion and the Boar \\r\\n \\r\\nA HOUND, whose only son was fond of martial L'\n",
            " b'Story:  \"You would\\r\\nnot have thought so,\" said the other, \"by the way you load him. \\r\\nWhy, you lazy old fellow,\" cried several tongues at once done.  He is\\r\\nmany of a Frog, who lived for the misfortunes of others.  \\r\\n \\r\\n\\r\\nThe Housedog to repite the earth, and the other\\r\\nwas the spoak of the Lion.  Let that monster go away and I\\r\\nwell as I carrying you, who do not assist in the chase, luxuritting to\\r\\ndivise thoughts proceeded from his seat, attempted to run away. \\r\\nThe Two Prosests the sun unquired howls were at\\r\\nan easy and being fell ond me.\"  \\r\\n \\r\\n\\r\\nMercury and the Sheep \\r\\n \\r\\nA BERDSMAN, forslaved by a Dog sell hime the theaters without before the\\r\\nwarm and farling the shepherd than all the early part of the seventeenth century.  In the year\\r\\n1610, a learned mesises, he said to him,\\r\\n\"Ah, you will have to walk along merry to certain the best of his stall beaten nearly\\r\\nand expressed his speckates in challeces.  \\r\\n \\r\\n\\r\\nThe Bald Knight \\r\\n \\r\\nA BALD KNIGHT, who wore a wig, went out to catch th'\n",
            " b'Story:  \"Late, Parted helped her, and the other\\r\\nwas the sover in addity of salt.  The Ass and at last by guileful\\r\\nspeeches succeedens, in the present day, to a full of amize, a\\r\\nsimilar resolved to the fear.\"  \\r\\n \\r\\n\\r\\nThe Lion and the Three Bulls \\r\\n \\r\\nAT ONE TITN was bearing to kill the Stag.  The\\r\\nwhole Body quickly learning white and excleaming\\r\\nit from the Ass, by witnessing his fate.\"  \\r\\n \\r\\nHappy is the more encouraged, believing them to be king.  The Jat dismiss the\\r\\nfables of the \"Eagle and the Fox\" and of the fables of the\\r\\n\"MArasma agaken, and the  afle often found the water painsed him:  \"When, I for thyself\\r\\nsile that being statue a wagon allow a comeans of imputeation, he was not able to\\r\\ndistinguishing differen, and the old man replied that they would\\r\\nnever have invented me wsialby by his fewlycession.  He inquired \"My grose a straig, benauted him out of the\\r\\nwater, fell in which he will be confisted and brought\\r\\nthem by the side of an old wall and\\r\\nwe to make a living by his t'\n",
            " b'Story:  what sit your song was apocater, and he\\r\\nsight to creat them towards the shepherd, came and sparrows huge of the\\r\\nwearings of live used him.  He thus addressed him:  \"Butt away as\\r\\nfast as they could to the middle of the neighborh.  He asked a Sharp temoth\\r\\nevenial I half point on the stage.  The Buffoon grunted and\\r\\ntiskens to Hercules, it is said he was not able to\\r\\ndistinguish one birds.  The Monkey answered the GEaferatus of\\r\\nSee are.\"  \\r\\n \\r\\nThe Horse choised and take a stripte, and wounded up\\r\\nfriends, to please she might obtain two eggs did it to\\r\\nweth.  The Traveler snaked his cast brinkee for her extraordinary aum\\r\\nsticks.  The Wild Goats were sone for a long time\\r\\nvery much bruised, sick, and unable to move.  A swarm of hungry\\r\\nblood-Wished flock and eat entorimed, in some fellows heard the\\r\\njournees, Parry and the Ora  Farmer for the sined and\\r\\nfless on a viset them up.  \\r\\n \\r\\n\\r\\nThe Wolves and the Sheep \\r\\n \\r\\n\"WON these fables are teethy from dehise his day\\r\\nunder the while t'], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.170729875564575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqBdvj4wrZLE",
        "outputId": "af409893-3d44-4841-e19f-a268fac3bad6"
      },
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7f5526612550>, because it is not built.\n",
            "WARNING:absl:Found untraced functions such as gru_cell_6_layer_call_fn, gru_cell_6_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "next_char = tf.constant(['Story:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(200):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGiHrHdfrak0",
        "outputId": "c49287c9-14e8-477f-d2cf-d98343d716f3"
      },
      "execution_count": 255,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Story:  \"Shallon will place\r\n",
            "you, were forgot into the hole.  The Snake wost loxt laiders sent\r\n",
            "quickly cried out toward him to be of good courage in the fourteenth\r\n",
            "century, when you make such revenged for\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have successfully managed to train a RNN model that can somewhat recreate stories based on what its trained on"
      ],
      "metadata": {
        "id": "njSj8xR_WQlR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F7WrCRjBWyAn"
      },
      "execution_count": 233,
      "outputs": []
    }
  ]
}